from nipype.interfaces.base import BaseInterface
from nipype.interfaces.base import BaseInterfaceInputSpec
from nipype.interfaces.base import CommandLineInputSpec
from nipype.interfaces.base import CommandLine
from nipype.interfaces.base import File
from nipype.interfaces.base import traits
from nipype.interfaces.base import TraitedSpec


# ==================================================================
"""
This function calculates additional DTI measures, i.e. AD and RD, that
FSL dtifi does not automatically generate
"""


class AdditionalDTIMeasuresInputSpec(BaseInterfaceInputSpec):
    L1 = File(exists=True, desc='First eigenvalue image', mandatory=True)
    L2 = File(exists=True, desc='Second eigenvalue image', mandatory=True)
    L3 = File(exists=True, desc='Third eigenvalue image', mandatory=True)


class AdditionalDTIMeasuresOutputSpec(TraitedSpec):
    AD = File(exists=True, desc="axial diffusivity (AD) image")
    RD = File(exists=True, desc="radial diffusivity (RD) image")


class AdditionalDTIMeasures(BaseInterface):
    input_spec = AdditionalDTIMeasuresInputSpec
    output_spec = AdditionalDTIMeasuresOutputSpec

    def _run_interface(self, runtime):
        import nibabel as nib
        from nipype.utils.filemanip import split_filename

        L1 = nib.load(self.inputs.L1).get_data()
        L2 = nib.load(self.inputs.L2).get_data()
        L3 = nib.load(self.inputs.L3).get_data()
        affine = nib.load(self.inputs.L1).get_affine()

        RD = (L2 + L3) / 2

        fname = self.inputs.L1
        _, base, _ = split_filename(fname)
        nib.save(nib.Nifti1Image(L1, affine), base + '_AD.nii.gz')
        nib.save(nib.Nifti1Image(RD, affine), base + '_RD.nii.gz')
        return runtime

    def _list_outputs(self):
        from nipype.utils.filemanip import split_filename
        import os
        outputs = self._outputs().get()
        fname = self.inputs.L1
        _, base, _ = split_filename(fname)
        outputs["AD"] = os.path.abspath(base + '_AD.nii.gz')
        outputs["RD"] = os.path.abspath(base + '_RD.nii.gz')
        return outputs

# ==================================================================
"""
Surface to volume transformation
This function calls FreeSurfer's aparc2aseg to transform surface annotations to an image volume.
"""


class Aparc2Aseg_InputSpec(BaseInterfaceInputSpec):
    annotation_file = traits.String(desc='annotation file', mandatory=True)
    hemi = traits.String(desc='hemisphere: lh or rh', mandatory=True)
    subjects_dir = traits.String(
        desc='FreeSurfer subject directory generated by recon-all', mandatory=True)
    subject_id = traits.String(
        desc='subject ID', mandatory=True)


class Aparc2Aseg_OutputSpec(TraitedSpec):
    subject_id = traits.String(desc='FreeSurfer subject ID')
    volume_parcellation = File(
        exists=True, desc="annotation file in target space")


class Aparc2Aseg(BaseInterface):
    input_spec = Aparc2Aseg_InputSpec
    output_spec = Aparc2Aseg_OutputSpec

    def _run_interface(self, runtime):
        import os
        from subprocess import call

        os.environ['SUBJECTS_DIR'] = self.inputs.subjects_dir

        if not os.path.isdir(self.inputs.subjects_dir + self.inputs.subject_id + '/parcellation/'):
            os.mkdir(self.inputs.subjects_dir +
                     self.inputs.subject_id + '/parcellation/')

        cmd = "mri_aparc2aseg --s " + self.inputs.subject_id + \
        " --annot " + self.inputs.annotation_file + \
        " --o " + self.inputs.subjects_dir + self.inputs.subject_id + \
            '/parcellation/' + self.inputs.annotation_file + '.nii.gz'
        " --rip-unknown --hypo-as-wm"

        call(cmd, shell=True)

        return runtime

    def _list_outputs(self):
        import os

        outputs = self._outputs().get()
        outputs['subject_id'] = self.inputs.subject_id
        outputs['volume_parcellation'] = os.path.abspath(
            self.inputs.subjects_dir + '/' + self.inputs.subject_id + '/parcellation/' + self.inputs.annotation_file + '.nii.gz')
        return outputs

# ==================================================================
"""
Extract statistics for surface parcellation
"""


class AparcStats_InputSpec(BaseInterfaceInputSpec):
    subjects_dir = traits.String(
        desc='FreeSurfer subject directory generated by recon-all', mandatory=True)
    subject_id = traits.String(
        desc='subject ID', mandatory=True)
    parcellation_name = traits.String(desc='parcellation name', mandatory=True)

class AparcStats_OutputSpec(TraitedSpec):
    lh_stats = File(
        exists=True, desc="statistics for regions in the left hemisphere")
    rh_stats = File(
        exists=True, desc="statistics for regions in the right hemisphere")
    parcellation_name = traits.String(desc='parcellation name', mandatory=True)


class AparcStats(BaseInterface):
    input_spec = AparcStats_InputSpec
    output_spec = AparcStats_OutputSpec

    def _run_interface(self, runtime):
        import os
        from subprocess import call

        subjects_dir = self.inputs.subjects_dir + '/'
        subject_id = self.inputs.subject_id
        parcellation_name = self.inputs.parcellation_name

        cmd = 'mris_anatomical_stats -a ' + subjects_dir + subject_id + '/label/lh.' + parcellation_name + '.annot -f ' + \
            subjects_dir + subject_id + '/stats/lh.' + parcellation_name + '.stats ' + subject_id + ' lh'
        call(cmd, shell=True)

        cmd = 'mris_anatomical_stats -a ' + subjects_dir + subject_id + '/label/rh.' + parcellation_name + '.annot -f ' + \
            subjects_dir + subject_id + '/stats/rh.' + parcellation_name + '.stats ' + subject_id + ' rh'
        call(cmd, shell=True)

        return runtime

    def _list_outputs(self):
        subjects_dir = self.inputs.subjects_dir
        subject_id = self.inputs.subject_id
        parcellation_name = self.inputs.parcellation_name

        outputs = self._outputs().get()
        outputs['lh_stats'] = subjects_dir + '/' +\
            subject_id + '/stats/lh.' + parcellation_name + '.stats'
        outputs['rh_stats'] = subjects_dir + '/' +\
            subject_id + '/stats/rh.' + parcellation_name + '.stats'
        outputs['parcellation_name'] =     parcellation_name

        return outputs


# ==================================================================
"""
Extract values for regions in an atlas (volume data)
"""


class AtlasValues_InputSpec(BaseInterfaceInputSpec):
    atlas_filename = File(exists=True, desc="filename of atlas data")
    morpho_filename = File(exists=True, desc="filename of morphometry image")


class AtlasValues_OutputSpec(TraitedSpec):
    atlas_values = File(
        exists=True, desc="CSV file containing values for each region in the atlas")


class AtlasValues(BaseInterface):
    input_spec = AtlasValues_InputSpec
    output_spec = AtlasValues_OutputSpec

    def _run_interface(self, runtime):
        import nibabel as nib
        from nipype.utils.filemanip import split_filename
        import numpy as np
        import pandas as pd

        atlas_filename = self.inputs.atlas_filename
        morpho_filename = self.inputs.morpho_filename

        atlas = nib.load(atlas_filename).get_data()
        morpho = nib.load(morpho_filename).get_data()
        results = pd.DataFrame()
        metric = morpho_filename.split('/')[-1].split('_')[1]
        atlas_name = atlas_filename.split('/')[-1].split('.nii.gz')[0]

        for region in np.unique(atlas):
            region_mask = np.asarray(atlas == region).astype('float64')
            results.set_value(
                region - 1, 'value', np.sum(morpho * region_mask) / np.sum(region_mask))

        _, base, _ = split_filename(morpho_filename)
        results.to_csv(base + '_' + metric + '_' + atlas_name + '.csv')

        return runtime

    def _list_outputs(self):
        from nipype.utils.filemanip import split_filename
        import os

        outputs = self._outputs().get()
        atlas_filename = self.inputs.atlas_filename
        morpho_filename = self.inputs.morpho_filename
        _, base, _ = split_filename(morpho_filename)
        metric = morpho_filename.split('/')[-1].split('_')[1]
        atlas_name = atlas_filename.split('/')[-1].split('.nii.gz')[0]
        atlas_name = atlas_filename.split('/')[-1].split('.nii.gz')[0]
        outputs['atlas_values'] = os.path.abspath(base + '_' + metric + '_' + atlas_name + '.csv')
        return outputs

# ======================================================================
# Calculate the connectivity matrix

class CalcMatrixInputSpec(BaseInterfaceInputSpec):
    track_file = File(
        exists=True, desc='whole-brain tractography in .trk format', mandatory=True)
    ROI_file = File(
        exists=True, desc='image containing the ROIs', mandatory=True)
    scalar_file = File(
        exists=True, desc='fractional anisotropy map in the same soace as the track file', mandatory=True)
    output_file = File(
        "scalar_matrix.txt", desc="Adjacency matrix of ROIs with scalar as conenction weight", usedefault=True)
    threshold = traits.Int(desc="Threshold of number of streamlines to retain")

class CalcMatrixOutputSpec(TraitedSpec):
    density_matrix = File(exists=True, desc="connectivity matrix based on the number of streamlines between each pair of ROIs")
    length_normalized_matrix = File(exists=True, desc="streamline density matrix normalized by the length of the streamlines between ROIs")
    scalar_matrix = File(exists=True, desc="connectivity matrix of scalar between each pair of ROIs")
    size_normalized_matrix = File(exists=True, desc="streamline density matrix normalized by the combined volume of the ROIs")


class CalcMatrix(BaseInterface):
    input_spec = CalcMatrixInputSpec
    output_spec = CalcMatrixOutputSpec

    def _run_interface(self, runtime):
        import nibabel as nib
        import numpy as np
        from dipy.tracking import utils
        from dipy.io.streamline import load_tractogram
        from dipy.tracking.streamline import Streamlines

        # Identity matrix affine
        affine = np.eye(4)

        # Loading the ROI file
        labels_img = nib.load(self.inputs.ROI_file)
        labels = labels_img.get_data()

        # Getting the scalar data
        scalar_img = nib.load(self.inputs.scalar_file)
        scalar_data = scalar_img.get_data()

        # Loading the streamlines
        streamlines = load_tractogram(self.inputs.track_file,
                                      reference=scalar_img,
                                      trk_header_check=True,
                                      bbox_valid_check=False)
        streamlines.to_rasmm()

        # Only keeping streamlines that pass through the ROIs
        ROI_mask = labels > 0
        target_streamlines = utils.target(streamlines.streamlines, target_mask=ROI_mask, affine=labels_img.affine)
        target_streamlines = Streamlines(target_streamlines)
        lengths = np.array([len(sl) for sl in target_streamlines])
        target_streamlines = target_streamlines[lengths > 1]

        # Constructing the streamlines matrix
        matrix, mapping = utils.connectivity_matrix(
            streamlines=target_streamlines,
            label_volume=labels.astype('int'),
            affine=labels_img.affine,
            symmetric=True,
            return_mapping=True,
            mapping_as_streamlines=True)

        matrix[matrix < self.inputs.threshold] = 0

        # Removing the background label
        matrix = matrix[1:, :]
        matrix = matrix[:, 1:]

        # Saving the density matrix
        from nipype.utils.filemanip import split_filename
        _, base, _ = split_filename(self.inputs.track_file)
        np.savetxt(base + '_' + str(self.inputs.threshold) + '_matrix.txt', matrix, delimiter='\t')

        # Density matrix normalized by ROI size
        ROI_sizes = [np.sum(labels[labels == ROI]) for ROI in np.unique(labels)[1:]]
        size_matrix = np.zeros(shape=np.repeat(len(np.unique(labels)[1:]),2))

        for ROI1 in range(0,len(np.unique(labels)[1:])):
            for ROI2 in range(0,len(np.unique(labels)[1:])):
                size_matrix[ROI1, ROI2] = ROI_sizes[ROI1] + ROI_sizes[ROI2]

        np.savetxt(base + '_' + str(self.inputs.threshold) + '_matrix_ROI_normalized.txt', matrix/size_matrix, delimiter='\t')

        # Density matrix normalized by streamline length
        length_matrix = np.zeros(shape=np.repeat(len(np.unique(labels)[1:]),2))

        for ROI1 in range(0,len(np.unique(labels)[1:])):
            for ROI2 in range(0,len(np.unique(labels)[1:])):
                length_matrix[ROI1, ROI2] = np.median(list(utils.length(mapping[ROI1, ROI2])))

        length_matrix[np.tril_indices(n=len(length_matrix))] = 0
        length_matrix = length_matrix.T + length_matrix - np.diagonal(length_matrix)

        np.savetxt(base + '_' + str(self.inputs.threshold) + '_matrix_length_normalized.txt', matrix/length_matrix, delimiter='\t')

        # Constructing the scalar matrix
        dimensions = matrix.shape
        scalar_matrix = np.empty(shape=dimensions)

        for i in range(0, dimensions[0]):
            for j in range(0, dimensions[1]):
                if matrix[i, j]:
                    dm = utils.density_map(
                        mapping[i, j], vol_dims=scalar_data.shape, affine=labels_img.affine)
                    scalar_matrix[i, j] = np.mean(scalar_data[dm > 5])
                else:
                    scalar_matrix[i, j] = 0

        scalar_matrix[np.tril_indices(n=len(scalar_matrix))] = 0
        scalar_matrix = scalar_matrix.T + scalar_matrix - np.diagonal(scalar_matrix)

        # Saving the scalar matrix
        from nipype.utils.filemanip import split_filename
        _, base, _ = split_filename(self.inputs.scalar_file)
        np.savetxt(base + '_matrix.txt', scalar_matrix, delimiter='\t')
        return runtime

    def _list_outputs(self):
        from nipype.utils.filemanip import split_filename
        import os
        outputs = self._outputs().get()
        _, base, _ = split_filename(self.inputs.scalar_file)
        outputs["scalar_matrix"] = os.path.abspath(base + '_matrix.txt')

        _, base, _ = split_filename(self.inputs.track_file)
        outputs["density_matrix"] = os.path.abspath(base + '_' + str(self.inputs.threshold) + '_matrix.txt')

        outputs["length_normalized_matrix"] = os.path.abspath(base + '_' + str(self.inputs.threshold) + '_matrix_length_normalized.txt')
        outputs["size_normalized_matrix"] = os.path.abspath(base + '_' + str(self.inputs.threshold) + '_matrix_ROI_normalized.txt')
        return outputs


# ==================================================================
# Write FreeSurfer parcellation results in separate CSV files

class FreeSurferValues_InputSpec(BaseInterfaceInputSpec):
    lh_filename =  File(desc='FreeSurfer stats file with morphometric values for left hemisphere', mandatory=True, exist=True)
    rh_filename =  File(desc='FreeSurfer stats file with morphometric values for right hemisphere', mandatory=True, exist=True)
    parcellation_name = traits.String(desc='name of the parcellation')

class FreeSurferValues_OutputSpec(TraitedSpec):
    SurfArea = File(exist=True, desc="CSV file with surface area values")
    GrayVol = File(dexist=True, esc="CSV file with gray matter area values")
    ThickAvg = File(exist=True, desc="CSV file with thickness average values")
    ThickStd = File(exist=True, desc="CSV file with thickness standard deviation valuese")
    MeanCurv = File(exist=True, desc="CSV file with mean curvature values")
    GausCurv = File(exist=True, desc="CSV file with gaussian curvature values")
    FoldInd = File(dexist=True, esc="CSV file with folding index values")
    CurvInd = File(dexist=True, esc="CSV file with curvature index values")

class FreeSurferValues(BaseInterface):
    input_spec = FreeSurferValues_InputSpec
    output_spec = FreeSurferValues_OutputSpec

    def _run_interface(self, runtime):
        import pandas as pd
        import re

        lh_filename = self.inputs.lh_filename
        rh_filename = self.inputs.rh_filename
        parcellation_name = self.inputs.parcellation_name

        SurfArea = pd.DataFrame()
        GrayVol = pd.DataFrame()
        ThickAvg = pd.DataFrame()
        ThickStd = pd.DataFrame()
        MeanCurv = pd.DataFrame()
        GausCurv = pd.DataFrame()
        FoldInd = pd.DataFrame()
        CurvInd = pd.DataFrame()

        index = 0

        for filename in [lh_filename, rh_filename]:
            file = open(filename, 'r')

            for line in file:
                if not re.search('#', line) and not re.search('unknown', line):
                    values = line.split()
                    SurfArea.set_value(index, 'value', values[1])
                    GrayVol.set_value(index, 'value', values[2])
                    ThickAvg.set_value(index, 'value', values[3])
                    ThickStd.set_value(index, 'value', values[4])
                    MeanCurv.set_value(index, 'value', values[5])
                    GausCurv.set_value(index, 'value', values[6])
                    FoldInd.set_value(index, 'value', values[7])
                    CurvInd.set_value(index, 'value', values[8])
                    index += 1

        SurfArea.to_csv(parcellation_name + '_SurfArea.csv')
        GrayVol.to_csv(parcellation_name + '_GrayVol.csv')
        ThickAvg.to_csv(parcellation_name + '_ThickAvg.csv')
        ThickStd.to_csv(parcellation_name + '_ThickStd.csv')
        MeanCurv.to_csv(parcellation_name + '_MeanCurv.csv')
        GausCurv.to_csv(parcellation_name + '_GausCurv.csv')
        FoldInd.to_csv(parcellation_name + '_FoldInd.csv')
        CurvInd.to_csv(parcellation_name + '_CurvInd.csv')

        return runtime

    def _list_outputs(self):
        import os

        outputs = self._outputs().get()
        lh_filename = self.inputs.lh_filename
        parcellation_name = self.inputs.parcellation_name

        outputs['SurfArea'] =  os.path.abspath(parcellation_name + '_SurfArea.csv')
        outputs['GrayVol'] =  os.path.abspath(parcellation_name + '_GrayVol.csv')
        outputs['ThickAvg'] =  os.path.abspath(parcellation_name + '_ThickAvg.csv')
        outputs['ThickStd'] =  os.path.abspath(parcellation_name + '_ThickStd.csv')
        outputs['MeanCurv'] =  os.path.abspath(parcellation_name + '_MeanCurv.csv')
        outputs['GausCurv'] =  os.path.abspath(parcellation_name + '_GausCurv.csv')
        outputs['FoldInd'] =  os.path.abspath(parcellation_name + '_FoldInd.csv')
        outputs['CurvInd'] =  os.path.abspath(parcellation_name + '_CurvInd.csv')

        return outputs

# ==================================================================
# Probabilistic trackign with a CSD model

class TractographyInputSpec(BaseInterfaceInputSpec):
    in_file = File(exists=True, desc='diffusion weighted volume', mandatory=True)
    bval = File(exists=True, desc='FSL-style b-value file', mandatory=True)
    bvec = File(exists=True, desc='FSL-style b-vector file', mandatory=True)
    FA = File(exists=True, desc='FA map', mandatory=True)
    brain_mask = File(exists=True, desc='FA map', mandatory=True)
    model = traits.String(desc='model to use for reconstruction, either CSA, CSD')

class TractographyOutputSpec(TraitedSpec):
    out_file = File(exists=True, desc="streamlines in NumPy format")
    out_track = File(exists=True, desc="tracks in Trackvis format")
    GFA = File(exist=True, desc="Generalized fractional anisotropy image")

class Tractography(BaseInterface):
    input_spec = TractographyInputSpec
    output_spec = TractographyOutputSpec

    def _run_interface(self, runtime):
        import numpy as np
        import nibabel as nib
        from dipy.core.gradients import gradient_table
        from nipype.utils.filemanip import split_filename
        from dipy.tracking.local_tracking import utils
        from dipy.reconst.shm import CsaOdfModel
        from dipy.data import default_sphere
        from dipy.direction import peaks_from_model
        from dipy.tracking.stopping_criterion import ThresholdStoppingCriterion
        from dipy.reconst.csdeconv import ConstrainedSphericalDeconvModel
        from dipy.reconst.csdeconv import auto_response
        from dipy.direction import DeterministicMaximumDirectionGetter
        from dipy.tracking.local_tracking import LocalTracking
        from dipy.io.streamline import save_tractogram
        from dipy.tracking.streamline import Streamlines
        from dipy.io.stateful_tractogram import Space, StatefulTractogram


        bvec = self.inputs.bvec
        bval = self.inputs.bval
        FA_fname = self.inputs.FA
        fname = self.inputs.in_file
        mask_fname = self.inputs.brain_mask
        model = self.inputs.model

        # Loading the data
        img = nib.load(fname)
        data = img.get_data()

        FA_img = nib.load(FA_fname)
        fa = FA_img.get_data()

        mask_img = nib.load(mask_fname)
        mask = mask_img.get_data()

        bval_fname = self.inputs.bval
        bvals = np.loadtxt(bval_fname)

        bvec_fname = self.inputs.bvec
        bvecs = np.loadtxt(bvec_fname)
        #bvecs = np.vstack([-1*bvecs[0,:],bvecs[1,:],bvecs[2,:]]).T
        bvecs = np.vstack([bvecs[0,:],bvecs[1,:],bvecs[2,:]]).T
        gtab = gradient_table(bvals, bvecs)

        # Creating a white matter mask
        fa = fa*mask
        white_matter = fa >= 0.2

        affine = np.eye(4)

        # Creating a seed mask
        seeds = utils.seeds_from_mask(white_matter, affine=affine, density=[1,1,1])

        # Fitting the CSA model
        csa_model = CsaOdfModel(gtab, sh_order=8)
        csa_peaks = peaks_from_model(csa_model, data, default_sphere,
                                     relative_peak_threshold=.8,
                                     min_separation_angle=30,
                                     mask=white_matter)
        classifier = ThresholdStoppingCriterion(csa_peaks.gfa, .1)

        if model == 'CSA':
            streamlines = LocalTracking(csa_peaks, classifier, seeds, np.eye(4), step_size=.5)

        if model == 'CSD':
            # CSD model
            from dipy.reconst.csdeconv import auto_response
            from dipy.reconst.csdeconv import ConstrainedSphericalDeconvModel
            from dipy.direction import ProbabilisticDirectionGetter


            response, ratio = auto_response(gtab, data, roi_radius=10, fa_thr=0.7)
            csd_model = ConstrainedSphericalDeconvModel(gtab, response, sh_order=8)
            csd_fit = csd_model.fit(data, mask=white_matter)

            prob_dg = ProbabilisticDirectionGetter.from_shcoeff(csd_fit.shm_coeff, max_angle=45., sphere=default_sphere)

            # Tracking
            streamlines = LocalTracking(prob_dg, classifier, seeds, affine,step_size=.5, max_cross=2)

        # Compute streamlines and store as a list.
        streamlines = Streamlines(streamlines)

        # Saving the tracks in NumPy format
        _, base, _ = split_filename(fname)
        np.save(base + '_' + self.inputs.model + '.npy', np.array(streamlines, dtype=np.object))

        # Saving the GFA image
        nib.save(nib.Nifti1Image(csa_peaks.gfa, FA_img.affine), base + '_GFA.nii.gz')

        # Saving the image for visualization in TrackVis
        dm = utils.density_map(streamlines, affine, fa.shape)
        dm_img = nib.Nifti1Image(dm.astype("int16"), FA_img.affine)

        # Save streamlines in Trackvis format
        streamlines_trk = Streamlines(streamlines)
        sft = StatefulTractogram(streamlines_trk, dm_img, Space.VOX)
        save_tractogram(sft, base + '_' + self.inputs.model + '.trk')

        return runtime

    def _list_outputs(self):
        from nipype.utils.filemanip import split_filename
        import os

        outputs = self._outputs().get()
        fname = self.inputs.in_file
        _, base, _ = split_filename(fname)
        outputs["out_file"] = os.path.abspath(base + '_' + self.inputs.model + '.npy')
        outputs["out_track"] = os.path.abspath(base + '_' + self.inputs.model +'.trk')
        outputs["GFA"] = os.path.abspath(base + '_GFA.nii.gz')
        return outputs

# ==================================================================
"""
Expand parcels
This function expands the parcellation labels into the white matter
"""

class ExpandParcelsInputSpec(BaseInterfaceInputSpec):
    dilatationVoxel = traits.Int(0, desc='Number of voxels to dilate the parcellation by', mandatory=True)
    parcellation_name = traits.String(
        desc='parcellation name', mandatory=True)
    parcellation_file = File(
        exists=True, desc='volume parcellation file', mandatory=True)
    subjects_dir = traits.String(
        desc='FreeSurfer subjects directory', mandatory=True)
    subject_id = traits.String(desc='subject ID', mandatory=True)
    white_matter_image = File(exists=True, desc='white matter image', mandatory=True)

class ExpandParcelsOutputSpec(TraitedSpec):
    out_file = File(desc="dilated parcellation image")
    subject_id = traits.String(desc='FreeSurfer subject ID')

class ExpandParcels(BaseInterface):
    input_spec = ExpandParcelsInputSpec
    output_spec = ExpandParcelsOutputSpec

    def _run_interface(self, runtime):
        import nibabel as nib
        import numpy as np
        from nipype.utils.filemanip import split_filename

        parcellation_file = self.inputs.parcellation_file
        white_matter_image = self.inputs.white_matter_image
        dilatationVoxel = self.inputs.dilatationVoxel
        parcellation_name = self.inputs.parcellation_name

        volGM = nib.load(parcellation_file).get_data()
        affine = nib.load(parcellation_file).affine
        volWM = nib.load(white_matter_image).get_data()
        wmCode = [2,41];

        # Selecting cortical areas
        newVol = volGM.copy()
        parcelSetlh = np.asarray(np.where(volGM == wmCode[0]))
        parcelSetrh = np.asarray(np.where(volGM == wmCode[1]))
        parcelWhite = np.hstack([parcelSetlh, parcelSetrh])

        parcelSet = np.unique(volGM);
        parcelSet = parcelSet[parcelSet > 1000]

        # Calculating the parcel centroids
        centroids = list()
        for parcel in parcelSet:
            ci,cj,ck = np.where(newVol == parcel)
            centroids.append([np.mean(ci), np.mean(cj), np.mean(ck)])

        centroids = np.asarray(centroids)

        for nv in range(0, np.shape(parcelWhite)[1]):
            ci,cj,ck = parcelWhite[..., nv]
            candidates = list()

            for ni in range(-dilatationVoxel, dilatationVoxel + 1):
                for nj in range(-dilatationVoxel, dilatationVoxel + 1):
                    for nk in range(-dilatationVoxel, dilatationVoxel + 1):
                        nci = ci + ni
                        ncj = cj + nj
                        nck = ck + nk
                        if (nci>0) & (nci<257) & (ncj>0) & (ncj<257) & (nck>0) & (nck<257):
                            if volGM[nci, ncj, nck] > 1000:
                                candidates.append(volGM[nci, ncj, nck])


            candidates = np.unique(candidates).transpose()
            distances = list()

            for nc in range(0, np.shape(candidates)[0]):
                candPos = np.where(parcelSet == candidates[nc])
                distances.append([((centroids[candPos, 0] - ci)**2)[0][0],
                 ((centroids[candPos, 1] - ci)**2)[0][0],
                  ((centroids[candPos, 2] - ci)**2)[0][0]])

            if distances:
                newVol[ci, cj, ck] = candidates[np.where(distances == np.min(distances))[0]]
            else:
                newVol[ci, cj, ck] = volGM[ci, cj, ck]

        path_subj = self.inputs.subjects_dir + self.inputs.subject_id
        nib.save(nib.Nifti1Image(newVol, affine),
                 self.inputs.subjects_dir + self.inputs.subject_id + '/parcellation/' + self.inputs.parcellation_name  + '_expanded.nii.gz')

        return runtime

    def _list_outputs(self):
        from nipype.utils.filemanip import split_filename
        import os
        outputs = self._outputs().get()
        outputs["out_file"] = os.path.abspath(self.inputs.subjects_dir + self.inputs.subject_id + '/parcellation/' + self.inputs.parcellation_name  + '_expanded.nii.gz')
        outputs["subject_id"] = self.inputs.subject_id
        return outputs



# ==================================================================
"""
Denoising with non-local means
This function is based on the example in the Dipy preprocessing tutorial:
http://nipy.org/dipy/examples_built/denoise_nlmeans.html#example-denoise-nlmeans
"""

class DipyDenoiseInputSpec(BaseInterfaceInputSpec):
    in_file = File(
        exists=True, desc='diffusion weighted volume for denoising', mandatory=True)


class DipyDenoiseOutputSpec(TraitedSpec):
    out_file = File(exists=True, desc="denoised diffusion-weighted volume")


class DipyDenoise(BaseInterface):
    input_spec = DipyDenoiseInputSpec
    output_spec = DipyDenoiseOutputSpec

    def _run_interface(self, runtime):
        import nibabel as nib
        import numpy as np
        from dipy.denoise.nlmeans import nlmeans
        from nipype.utils.filemanip import split_filename

        fname = self.inputs.in_file
        img = nib.load(fname)
        data = img.get_data()
        affine = img.get_affine()
        mask = data[..., 0] > 80
        a = data.shape

        denoised_data = np.ndarray(shape=data.shape)
        for image in range(0, a[3]):
            print(str(image + 1) + '/' + str(a[3] + 1))
            dat = data[..., image]
            # Calculating the standard deviation of the noise
            sigma = np.std(dat[~mask])
            den = nlmeans(dat, sigma=sigma, mask=mask)
            denoised_data[:, :, :, image] = den

        _, base, _ = split_filename(fname)
        nib.save(nib.Nifti1Image(denoised_data, affine),
                 base + '_denoised.nii.gz')

        return runtime

    def _list_outputs(self):
        from nipype.utils.filemanip import split_filename
        import os
        outputs = self._outputs().get()
        fname = self.inputs.in_file
        _, base, _ = split_filename(fname)
        outputs["out_file"] = os.path.abspath(base + '_denoised.nii.gz')
        return outputs


# ==================================================================
"""
Denoising with non-local means for 3D images
This function is based on the example in the Dipy preprocessing tutorial:
http://nipy.org/dipy/examples_built/denoise_nlmeans.html#example-denoise-nlmeans
"""


class DipyDenoiseT1InputSpec(BaseInterfaceInputSpec):
    in_file = File(
        exists=True, desc='diffusion weighted volume for denoising', mandatory=True)


class DipyDenoiseT1OutputSpec(TraitedSpec):
    out_file = File(exists=True, desc="denoised diffusion-weighted volume")


class DipyDenoiseT1(BaseInterface):
    input_spec = DipyDenoiseT1InputSpec
    output_spec = DipyDenoiseT1OutputSpec

    def _run_interface(self, runtime):
        import nibabel as nib
        import numpy as np
        from dipy.denoise.nlmeans import nlmeans
        from nipype.utils.filemanip import split_filename

        fname = self.inputs.in_file
        img = nib.load(fname)
        data = img.get_data()
        affine = img.get_affine()
        mask = data > 20

        # Calculating the standard deviation of the noise
        sigma = np.std(data[~mask])
        denoised_data = nlmeans(data, sigma=sigma, mask=mask)

        _, base, _ = split_filename(fname)
        nib.save(nib.Nifti1Image(denoised_data, affine),
                 base + '_denoised.nii')

        return runtime

    def _list_outputs(self):
        from nipype.utils.filemanip import split_filename
        import os
        outputs = self._outputs().get()
        fname = self.inputs.in_file
        _, base, _ = split_filename(fname)
        outputs["out_file"] = os.path.abspath(base + '_denoised.nii')
        return outputs

# ==================================================================
"""
Expand parcels
This function expands the parcellation labels into the white matter
"""

class ExpandParcelsInputSpec(BaseInterfaceInputSpec):
    dilatationVoxel = traits.Int(0, desc='Number of voxels to dilate the parcellation by', mandatory=True)
    parcellation_name = traits.String(
        desc='parcellation name', mandatory=True)
    parcellation_file = File(
        exists=True, desc='volume parcellation file', mandatory=True)
    subjects_dir = traits.String(
        desc='FreeSurfer subjects directory', mandatory=True)
    subject_id = traits.String(desc='subject ID', mandatory=True)
    white_matter_image = File(exists=True, desc='white matter image', mandatory=True)

class ExpandParcelsOutputSpec(TraitedSpec):
    out_file = File(desc="dilated parcellation image")
    subject_id = traits.String(desc='FreeSurfer subject ID')

class ExpandParcels(BaseInterface):
    input_spec = ExpandParcelsInputSpec
    output_spec = ExpandParcelsOutputSpec

    def _run_interface(self, runtime):
        import nibabel as nib
        import numpy as np
        from nipype.utils.filemanip import split_filename

        parcellation_file = self.inputs.parcellation_file
        white_matter_image = self.inputs.white_matter_image
        dilatationVoxel = self.inputs.dilatationVoxel
        parcellation_name = self.inputs.parcellation_name

        volGM = nib.load(parcellation_file).get_data()
        affine = nib.load(parcellation_file).affine
        volWM = nib.load(white_matter_image).get_data()
        wmCode = [2,41];

        # Selecting cortical areas
        newVol = volGM.copy()
        parcelSetlh = np.asarray(np.where(volGM == wmCode[0]))
        parcelSetrh = np.asarray(np.where(volGM == wmCode[1]))
        parcelWhite = np.hstack([parcelSetlh, parcelSetrh])

        parcelSet = np.unique(volGM);
        parcelSet = parcelSet[parcelSet > 1000]

        # Calculating the parcel centroids
        centroids = list()
        for parcel in parcelSet:
            ci,cj,ck = np.where(newVol == parcel)
            centroids.append([np.mean(ci), np.mean(cj), np.mean(ck)])

        centroids = np.asarray(centroids)

        for nv in range(0, np.shape(parcelWhite)[1]):
            ci,cj,ck = parcelWhite[..., nv]
            candidates = list()

            for ni in range(-dilatationVoxel, dilatationVoxel + 1):
                for nj in range(-dilatationVoxel, dilatationVoxel + 1):
                    for nk in range(-dilatationVoxel, dilatationVoxel + 1):
                        nci = ci + ni
                        ncj = cj + nj
                        nck = ck + nk
                        if (nci>0) & (nci<257) & (ncj>0) & (ncj<257) & (nck>0) & (nck<257):
                            if volGM[nci, ncj, nck] > 1000:
                                candidates.append(volGM[nci, ncj, nck])


            candidates = np.unique(candidates).transpose()
            distances = list()

            for nc in range(0, np.shape(candidates)[0]):
                candPos = np.where(parcelSet == candidates[nc])
                distances.append([((centroids[candPos, 0] - ci)**2)[0][0],
                 ((centroids[candPos, 1] - ci)**2)[0][0],
                  ((centroids[candPos, 2] - ci)**2)[0][0]])

            if distances:
                newVol[ci, cj, ck] = candidates[np.where(distances == np.min(distances))[0]]
            else:
                newVol[ci, cj, ck] = volGM[ci, cj, ck]

        path_subj = self.inputs.subjects_dir + self.inputs.subject_id
        nib.save(nib.Nifti1Image(newVol, affine),
                 self.inputs.subjects_dir + '/' + self.inputs.subject_id + '/parcellation/' + self.inputs.parcellation_name  + '_expanded.nii.gz')

        return runtime

    def _list_outputs(self):
        from nipype.utils.filemanip import split_filename
        import os
        outputs = self._outputs().get()
        outputs["out_file"] = os.path.abspath(self.inputs.subjects_dir + '/' +self.inputs.subject_id + '/parcellation/' + self.inputs.parcellation_name  + '_expanded.nii.gz')
        outputs["subject_id"] = self.inputs.subject_id
        return outputs


# ======================================================================
# Extract b0

class Extractb0InputSpec(BaseInterfaceInputSpec):
    in_file = File(
        exists=True, desc='diffusion-weighted image (4D)', mandatory=True)


class Extractb0OutputSpec(TraitedSpec):
    out_file = File(exists=True, desc="First volume of the dwi file")


class Extractb0(BaseInterface):
    input_spec = Extractb0InputSpec
    output_spec = Extractb0OutputSpec

    def _run_interface(self, runtime):
        import nibabel as nib
        img = nib.load(self.inputs.in_file)
        data = img.get_data()
        affine = img.get_affine()

        from nipype.utils.filemanip import split_filename
        import os
        outputs = self._outputs().get()
        fname = self.inputs.in_file
        _, base, _ = split_filename(fname)
        nib.save(nib.Nifti1Image(data[..., 0], affine),
                 os.path.abspath(base + '_b0.nii.gz'))
        return runtime

    def _list_outputs(self):
        from nipype.utils.filemanip import split_filename
        import os
        outputs = self._outputs().get()
        fname = self.inputs.in_file
        _, base, _ = split_filename(fname)
        outputs["out_file"] = os.path.abspath(base + '_b0.nii.gz')
        return outputs


# ==================================================================
"""
Rename files when a skull-stripped image is used in the recon-all pipeline
"""

class FSRenameInputSpec(BaseInterfaceInputSpec):
    subjects_dir = traits.String(
        desc='FreeSurfer subjects directory', mandatory=True)
    subject_id = traits.String(desc='subject ID', mandatory=True)


class FSRenameOutputSpec(TraitedSpec):
    brainmaskauto = File(exists=True, desc="thresholded volume")
    brainmask = File(exists=True, desc="thresholded volume")


class FSRename(BaseInterface):
    input_spec = FSRenameInputSpec
    output_spec = FSRenameOutputSpec

    def _run_interface(self, runtime):
        import shutil

        subjects_dir = self.inputs.subjects_dir
        subject_id = self.inputs.subject_id

        shutil.copyfile(subjects_dir + '/' + subject_id + '/mri/T1.mgz',
                        subjects_dir + '/' + subject_id + '/mri/brainmask.auto.mgz')

        shutil.copyfile(subjects_dir + '/' + subject_id + '/mri/T1.mgz', subjects_dir + '/' + subject_id + '/mri/brainmask.mgz')

        return runtime

    def _list_outputs(self):
        import os

        outputs = self._outputs().get()
        subjects_dir = self.inputs.subjects_dir
        subject_id = self.inputs.subject_id
        outputs["brainmaskauto"] = os.path.abspath(
           subjects_dir + '/' + subject_id + '/mri/brainmask.auto.mgz')
        outputs["brainmask"] = os.path.abspath(subjects_dir + '/' + subject_id  + '/mri/brainmask.mgz')
        return outputs

# ==================================================================
"""
Renumber parcels
This function renumbers the parcels and creates maps for cortical and subcortical parcellation
"""
class ReunumberParcelsInputSpec(BaseInterfaceInputSpec):
    subjects_dir = traits.String(
        desc='FreeSurfer subject directory generated by recon-all', mandatory=True)
    subject_id = traits.String(
        desc='subject ID', mandatory=True)
    parcellation_name = traits.String(
        desc='parcellation name', mandatory=True)

class ReunumberParcelsOutputSpec(TraitedSpec):
    cortical = File(exists=True, desc="cortical parcellation")
    cortical_consecutive = File(exists=True, desc="cortical parcellation with consecutive numbering")
    cortical_expanded = File(exists=True, desc="cortical parcellation expanded into WM")
    cortical_expanded_consecutive = File(exists=True, desc="cortical parcellation expanded into WM with consecutive numbering")
    leftHemisphere = File(exists=True, desc="left hemisphere parcellation")
    leftHemisphere_expanded = File(exists=True, desc="left hemisphere parcellation expanded into WM")
    orig = File(exists=True, desc="original parcellation image")
    renum = File(exists=True, desc="renumbered parcellation")
    renum_expanded = File(exists=True, desc="renumbered parcellation expanded into WM")
    renum_subMask = File(exists=True, desc="renumbered parcellation with subcortical regions masked out")
    rightHemisphere = File(exists=True, desc="parcellation of the right hemisphere")
    rightHemisphere_expanded = File(exists=True, desc="parcellation of the right hemisphere expanded into WM")
    subcortical = File(exists=True, desc="parcellation of subcortical regions")
    subcortical_expanded = File(exists=True, desc="parcellation of subcortical regions expanded into WM")
    whiteMatter = File(exists=True, desc="white matter partial volume")
    whiteMatter_expanded = File(exists=True, desc="white matter partial image after expansion of cortical parcellation into WM")
    boundary_lh_rh = File(exists=True, desc="boundary label between hemisphere")
    boundary_sub_lh = File(exists=True, desc="oundary label between cortical and subcortical")
    aparc = File(exists=True, desc="DK atlas")
    aparc_subMask = File(exists=True, desc="DK atlas with subcortical regions masked out")

class ReunumberParcels(BaseInterface):
    input_spec = ReunumberParcelsInputSpec
    output_spec = ReunumberParcelsOutputSpec

    def _run_interface(self, runtime):
        import nibabel as nib
        import numpy as np
        import os
        from subprocess import call

        wmpar1 = 1
        wmpar2 = 20
        path_subj = self.inputs.subjects_dir + '/' + self.inputs.subject_id + '/'
        parcellation_name = self.inputs.parcellation_name

        vol = nib.load(path_subj + 'parcellation/' +
                       parcellation_name + '.nii.gz').get_data()
        affine = nib.load(path_subj + 'parcellation/' +
                          parcellation_name + '.nii.gz').affine
        nib.save(nib.Nifti1Image(vol, affine), path_subj +
                 'parcellation/' + parcellation_name + '_orig.nii.gz')

        vol[vol == 72] = 31
        vol[vol == 80] = 0
        vol[vol == 29] = 0
        nib.save(nib.Nifti1Image(vol, affine), path_subj +
                 'parcellation/' + parcellation_name + '.nii.gz')

        volRenum = np.zeros(shape=vol.shape)
        volRenum_exp = np.zeros(shape=vol.shape)
        parcels = np.unique(vol)
        parcels_sub = parcels[parcels < 1000]

        if len(parcels_sub) < 42:
            pos_OpticChiasm = np.where(vol == 85)
            vol[pos_OpticChiasm[0], pos_OpticChiasm[1], pos_OpticChiasm[2]] = 77
            parcels = np.unique(vol)
            parcels_sub = parcels[parcels < 1000]
            if len(parcels_sub) < 42:
                print('Less subcortical regions than expected')

        numParcels = parcels.shape[0]
        limitSub_array = np.where(parcels < 1001)[0]
        limitSub = limitSub_array[-1]
        call('echo ' + str(limitSub) + ' >' + path_subj +
             'parcellation/boundary_sub_lh.txt', shell=True)

        limitHemi_array = np.where(parcels < 2001)[0]
        limitHemi = limitHemi_array[-1]
        call('echo ' + str(limitHemi) + ' >' + path_subj +
             'parcellation/boundary_lh_rh.txt', shell=True)

        for type in ['_expanded', '']:
            vol = nib.load(path_subj + 'parcellation/' +
                           parcellation_name + type + '.nii.gz').get_data()
            affine = nib.load(path_subj + 'parcellation/' +
                              parcellation_name + type + '.nii.gz').affine

            vol[vol == 72] = 31
            vol[vol == 80] = 0
            vol[vol == 29] = 0
            nib.save(nib.Nifti1Image(vol, affine), path_subj +
                     'parcellation/' + parcellation_name + type + '.nii.gz')

            for i in range(1, numParcels):
                voxParcel = np.where(vol == parcels[i])
                volRenum[voxParcel[0], voxParcel[1], voxParcel[2]] = i

            nib.save(nib.Nifti1Image(volRenum, affine), path_subj +
                     'parcellation/' + parcellation_name + '_renum' + type + '.nii.gz')

            # Renum start in 1
            vol_Renumst = volRenum.copy()
            vol_Renumst = vol_Renumst - limitSub
            vol_Renumst[vol_Renumst < 0] = 0
            nib.save(nib.Nifti1Image(vol_Renumst, affine), path_subj + 'parcellation/' +
                     parcellation_name + '_cortical' + type + '_consecutive.nii.gz')

            # Renum parts
            vol = volRenum.copy()

            # Subcortical
            newVol = vol.copy()
            newVol[newVol > limitSub] = 0
            newVol[newVol == wmpar1] = 0
            newVol[newVol == wmpar2] = 0
            nib.save(nib.Nifti1Image(newVol, affine), path_subj + 'parcellation/' +
                     parcellation_name + '_subcortical' + type + '.nii.gz')

            # Cortical
            newVol = vol.copy()
            newVol[newVol < limitSub + 1] = 0
            newVol[newVol == wmpar1] = 0
            newVol[newVol == wmpar2] = 0
            nib.save(nib.Nifti1Image(newVol, affine), path_subj +
                     'parcellation/' + parcellation_name + '_cortical' + type + '.nii.gz')

            # Cortical start in 1
            newVol = vol.copy()
            newVol[newVol < limitSub + 1] = 0
            newVol[newVol == wmpar1] = 0
            newVol[newVol == wmpar2] = 0
            newVol = newVol - limitSub
            newVol[newVol < 0] = 0
            nib.save(nib.Nifti1Image(newVol, affine), path_subj + 'parcellation/' +
                     parcellation_name + '_cortical' + type + '_consecutive.nii.gz')

            # Left
            newVol = vol.copy()
            newVol[newVol < limitSub + 1] = 0
            newVol[newVol > limitHemi] = 0
            nib.save(nib.Nifti1Image(newVol, affine), path_subj + 'parcellation/' +
                     parcellation_name + '_leftHemisphere' + type + '.nii.gz')

            # Right
            newVol = vol.copy()
            newVol[newVol < limitHemi + 1] = 0
            nib.save(nib.Nifti1Image(newVol, affine), path_subj + 'parcellation/' +
                     parcellation_name + '_rightHemisphere' + type + '.nii.gz')
            # white matter
            newVol = np.zeros(np.shape(vol))
            newVol[vol == wmpar1] = wmpar1
            newVol[vol == wmpar2] = wmpar2
            nib.save(nib.Nifti1Image(newVol, affine), path_subj + 'parcellation/' +
                     parcellation_name + '_whiteMatter' + type + '.nii.gz')

            # masking subcortical
            maskRegions = [1, 2, 3, 4, 5, 10, 11, 12, 15, 18, 19,
                           20, 21, 22, 23, 24, 33, 34, 35, 36, 37, 38, 39, 40, 41]

            volNew = volRenum.copy()
            ind = 1

            for Region in np.unique(volRenum)[1:]:
                if Region in maskRegions:
                    volNew[volNew == Region] = 0
                else:
                    volNew[volNew == Region] = ind
                    ind += 1

            nib.save(nib.Nifti1Image(volNew, affine), path_subj + 'parcellation/' +
                     parcellation_name + '_renum_subMask' + type + '.nii.gz')

        # masking subcortical for DK atlas
        maskRegions = np.hstack([range(1, 8), range(14, 16), range(
            19, 25), 25, range(29, 47), range(55, 57), 59, range(61, 999)])

        cmd = 'mri_aparc2aseg --s ' + path_subj.split('/')[-2] + ' --o ' + path_subj + 'parcellation/' + \
            'aparc.a2009s.nii.gz  --annot  aparc.a2009s  --rip-unknown --hypo-as-wm'

        call(cmd, shell=True)

        vol = nib.load(path_subj + 'parcellation/' +
                       'aparc.a2009s.nii.gz').get_data()
        affine = nib.load(path_subj + 'parcellation/' +
                          'aparc.a2009s.nii.gz').affine

        vol[vol == 72] = 31
        vol[vol == 80] = 0
        vol[vol == 29] = 0
        nib.save(nib.Nifti1Image(vol, affine), path_subj +
                 'parcellation/' + 'aparc.a2009s.nii.gz')

        maskRegions = np.hstack([range(1, 9), range(14, 17), range(
            19, 26), 27, range(29, 48), range(55, 58), 59, range(61, 1000)])

        volNew = vol.copy()
        ind = 1

        for Region in np.unique(vol)[1:]:
            if Region in maskRegions:
                volNew[volNew == Region] = 0
            else:
                volNew[volNew == Region] = ind
                ind += 1

        nib.save(nib.Nifti1Image(volNew, affine), path_subj +
                 'parcellation/aparc.a2009s_subMask.nii.gz')

        return runtime

    def _list_outputs(self):
        from nipype.utils.filemanip import split_filename
        import os
        outputs = self._outputs().get()
        path_subj = self.inputs.subjects_dir + '/' + self.inputs.subject_id + '/'
        parcellation_name = self.inputs.parcellation_name

        outputs["cortical"] = os.path.abspath(path_subj + 'parcellation/' + parcellation_name + '_cortical.nii.gz')
        outputs["cortical_consecutive"] = os.path.abspath(path_subj + 'parcellation/' + parcellation_name + '_cortical_consecutive.nii.gz')
        outputs["cortical_expanded"] = os.path.abspath(path_subj + 'parcellation/' + parcellation_name + '_cortical_expanded.nii.gz')
        outputs["cortical_expanded_consecutive"] = os.path.abspath(path_subj + 'parcellation/' + parcellation_name +  '_cortical_expanded_consecutive.nii.gz')
        outputs["leftHemisphere"] = os.path.abspath(path_subj + 'parcellation/' + parcellation_name + '_leftHemisphere.nii.gz')
        outputs["leftHemisphere_expanded"] = os.path.abspath(path_subj + 'parcellation/' + parcellation_name + '_leftHemisphere_expanded.nii.gz')
        outputs["orig"] = os.path.abspath(path_subj + 'parcellation/' + parcellation_name + '_orig.nii.gz')
        outputs["renum"] = os.path.abspath(path_subj + 'parcellation/' + parcellation_name + '_renum.nii.gz')
        outputs["renum_expanded"] = os.path.abspath(path_subj + 'parcellation/' + parcellation_name + '_renum_expanded.nii.gz')
        outputs["renum_subMask"] = os.path.abspath(path_subj + 'parcellation/' + parcellation_name + '_renum_subMask.nii.gz')
        outputs["rightHemisphere"] = os.path.abspath(path_subj + 'parcellation/' + parcellation_name + '_rightHemisphere.nii.gz')
        outputs["rightHemisphere_expanded"] = os.path.abspath(path_subj + 'parcellation/' + parcellation_name + '_rightHemisphere_expanded.nii.gz')
        outputs["subcortical"] = os.path.abspath(path_subj + 'parcellation/' +parcellation_name +  '_subcortical.nii.gz')
        outputs["subcortical_expanded"] = os.path.abspath(path_subj + 'parcellation/' + parcellation_name +  '_subcortical_expanded.nii.gz')
        outputs["whiteMatter"] = os.path.abspath(path_subj + 'parcellation/' + parcellation_name + '_whiteMatter.nii.gz')
        outputs["whiteMatter_expanded"] = os.path.abspath(path_subj + 'parcellation/' + parcellation_name + '_whiteMatter_expanded.nii.gz')
        outputs["boundary_lh_rh"] = os.path.abspath(path_subj + 'parcellation/' + 'boundary_lh_rh.txt')
        outputs["boundary_sub_lh"] = os.path.abspath(path_subj + 'parcellation/' + 'boundary_sub_lh.txt')
        outputs["aparc"] = os.path.abspath(path_subj + 'parcellation/' + 'aparc.a2009s.nii.gz')
        outputs["aparc_subMask"] = os.path.abspath(path_subj + 'parcellation/' + 'aparc.a2009s_subMask.nii.gz')
        return outputs


# ==================================================================
# Surface to surface transform using FreeSurfer (the native NiPyPe solution has a bug)

class SurfaceTransform_InputSpec(BaseInterfaceInputSpec):
    hemi = traits.String(desc='hemisphere: lh or rh', mandatory=True)
    source_annot_file = traits.String(desc='source annotation file', mandatory=True)
    source_subject = traits.String(desc='source subject ID', mandatory=True)
    subjects_dir = File(desc='path for FreeSurfer subjects directory', mandatory=True, exist=True)
    subject_id = traits.String(
        desc='FreeSurfer subject ID', mandatory=True)
    target_subject = traits.String(desc='target subject ID', mandatory=True)

class SurfaceTransform_OutputSpec(TraitedSpec):
    trgsurf = File(
        exists=True, desc="annotation file in target space")
    subject_id = traits.String(desc='subject ID')

class SurfaceTransform(BaseInterface):
    input_spec = SurfaceTransform_InputSpec
    output_spec = SurfaceTransform_OutputSpec

    def _run_interface(self, runtime):
        import os
        from subprocess import call

        os.environ['SUBJECTS_DIR'] = self.inputs.subjects_dir
        cmd = "mri_surf2surf --hemi " + self.inputs.hemi + \
        " --srcsubject " + self.inputs.source_subject + \
        " --sval-annot " + self.inputs.source_annot_file + \
        " --trgsubject " + self.inputs.target_subject + \
        " --trgsurfval " + self.inputs.source_annot_file
        call(cmd, shell=True)

        return runtime

    def _list_outputs(self):
        import os

        outputs = self._outputs().get()
        subject_directory = self.inputs.subjects_dir
        target_subject = self.inputs.target_subject
        hemisphere = self.inputs.hemi
        annotation_file = self.inputs.source_annot_file
        outputs['trgsurf'] = os.path.abspath(
            subject_directory + '/' + target_subject + '/label/' + hemisphere + '.' + annotation_file + '.annot')
        outputs['subject_id'] = self.inputs.subject_id
        return outputs
